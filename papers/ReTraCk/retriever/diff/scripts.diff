diff --git scripts/generate_candidates.py scripts/generate_candidates.py
index 1ff30b5..8232e7f 100644
--- scripts/generate_candidates.py
+++ scripts/generate_candidates.py
@@ -4,18 +4,17 @@
 # This source code is licensed under the license found in the
 # LICENSE file in the root directory of this source tree.
 #
+import sys
 import torch
 from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset
 from elq.biencoder.biencoder import load_biencoder
 import elq.candidate_ranking.utils as utils
 import json
-import sys
 import os
 from tqdm import tqdm
 
 import argparse
 
-
 def encode_candidate(
     reranker,
     candidate_pool,
@@ -39,8 +38,10 @@ def encode_candidate(
     cand_encode_list = None
     for step, batch in enumerate(iter_):
         cands = batch
+        print(cands.shape)
         cands = cands.to(device)
         cand_encode = reranker.encode_candidate(cands)
+        print(cand_encode.shape)
         if cand_encode_list is None:
             cand_encode_list = cand_encode
         else:
@@ -57,14 +58,16 @@ def load_candidate_pool(
 ):
     candidate_pool = None
     # try to load candidate pool from file
+    
     try:
         logger.info("Loading pre-generated candidate pool from: ")
         logger.info(cand_pool_path)
         candidate_pool = torch.load(cand_pool_path)
-    except:
+    except Exception as e:
+        print (e)
         logger.info("Loading failed.")
     assert candidate_pool is not None
-
+    
     return candidate_pool
 
 
@@ -102,18 +105,26 @@ except json.decoder.JSONDecodeError:
 biencoder_params["path_to_model"] = args.path_to_model
 # entities to use
 biencoder_params["entity_dict_path"] = args.entity_dict_path
-biencoder_params["degug"] = False
+biencoder_params["debug"] = False
 biencoder_params["data_parallel"] = True
 biencoder_params["no_cuda"] = False
-biencoder_params["max_context_length"] = 32
+biencoder_params["max_context_length"] = 150
 biencoder_params["encode_batch_size"] = args.batch_size
-
+biencoder_params["bert_model"] = "bert-base-uncased"
+#biencoder_params["lowercase"] = True
+biencoder_params["load_cand_enc_only"] = False
+#biencoder_params["out_dim"]=1
+#biencoder_params["pull_from_layer"] = -1
+#biencoder_params["add_linear"] = False
+#biencoder_params["silent"] = False
+#biencoder_params["debug"] = False
 saved_cand_ids = getattr(args, 'saved_cand_ids', None)
 encoding_save_file_dir = args.encoding_save_file_dir
 if encoding_save_file_dir is not None and not os.path.exists(encoding_save_file_dir):
     os.makedirs(encoding_save_file_dir, exist_ok=True)
 
 logger = utils.get_logger(biencoder_params.get("model_output_path", None))
+print(biencoder_params)
 biencoder = load_biencoder(biencoder_params)
 baseline_candidate_encoding = None
 if getattr(args, 'compare_saved_embeds', None) is not None:
@@ -151,7 +162,7 @@ candidate_encoding = encode_candidate(
 if save_file is not None:
     torch.save(candidate_encoding, save_file)
 
-print(candidate_encoding[0,:10])
+
 if baseline_candidate_encoding is not None:
     print(baseline_candidate_encoding[0,:10])
 
